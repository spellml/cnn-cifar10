{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wandb\n",
    "\n",
    "This notebook is a further refinement of the model defined in `train.ipynb` that replaces TensorBoard with Spell's [Weights & Biases integration](https://spell.ml/docs/integrating_wandb/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../models/wandb_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../models/wandb_train.py\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "from spell.metrics import send_metric\n",
    "\n",
    "import re\n",
    "import os\n",
    "CWD = os.environ[\"PWD\"]\n",
    "if not os.path.exists(f\"{CWD}/checkpoints/\"):\n",
    "    os.mkdir(f\"{CWD}/checkpoints/\")\n",
    "writer = SummaryWriter(f\"{CWD}/tensorboard/\")\n",
    "\n",
    "import wandb\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', type=int, dest='epochs', default=20)\n",
    "parser.add_argument('--batch_size', type=int, dest='batch_size', default=32)\n",
    "\n",
    "parser.add_argument('--conv1_filters', type=int, dest='conv1_filters', default=32)\n",
    "parser.add_argument('--conv2_filters', type=int, dest='conv2_filters', default=64)\n",
    "parser.add_argument('--dense_layer', type=int, dest='dense_layer', default=512)\n",
    "\n",
    "parser.add_argument('--conv1_dropout', type=float, dest='conv1_dropout', default=0.25)\n",
    "parser.add_argument('--conv2_dropout', type=float, dest='conv2_dropout', default=0.25)\n",
    "parser.add_argument('--dense_dropout', type=float, dest='dense_dropout', default=0.5)\n",
    "\n",
    "parser.add_argument('--from_checkpoint', type=str, dest='from_checkpoint', default=\"\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Used for testing purposes.\n",
    "# class Args:\n",
    "#     def __init__(self):\n",
    "#         self.epochs = 50\n",
    "#         self.batch_size = 32\n",
    "#         self.conv1_filters = 32\n",
    "#         self.conv2_filters = 64\n",
    "#         self.dense_layer = 512\n",
    "#         self.conv1_dropout = 0.25\n",
    "#         self.conv2_dropout = 0.25\n",
    "#         self.dense_dropout = 0.5\n",
    "#         self.from_checkpoint = False\n",
    "# args = Args()\n",
    "\n",
    "transform_train = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    # torchvision.transforms.Lambda(lambda x: torch.tensor(np.array(x).reshape((3, 32, 32)) / 255, dtype=torch.float)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "transform_test = torchvision.transforms.Compose([\n",
    "    # torchvision.transforms.Lambda(lambda x: torch.tensor(np.array(x).reshape((3, 32, 32)) / 255, dtype=torch.float)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "download = not os.path.exists(\"/mnt/cifar10/\")\n",
    "if download:\n",
    "    print(\"CIFAR10 dataset not on disk, downloading...\")\n",
    "else:\n",
    "    print(\"CIFAR10 dataset is already on disk! Skipping download.\")\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\"/mnt/cifar10/\", train=True, transform=transform_train, download=download)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "# download=False because the train and test sets are downloaded simultaneously\n",
    "test_dataset = torchvision.datasets.CIFAR10(\"/mnt/cifar10/\", train=False, transform=transform_test, download=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "class CIFAR10Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn_block_1 = nn.Sequential(*[\n",
    "            nn.Conv2d(3, args.conv1_filters, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(args.conv1_filters, args.conv2_filters, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(args.conv1_dropout)\n",
    "        ])\n",
    "        self.cnn_block_2 = nn.Sequential(*[\n",
    "            nn.Conv2d(args.conv2_filters, args.conv2_filters, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(args.conv2_filters, args.conv2_filters, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(args.conv2_dropout)\n",
    "        ])\n",
    "        self.flatten = lambda inp: torch.flatten(inp, 1)\n",
    "        self.head = nn.Sequential(*[\n",
    "            nn.Linear(args.conv2_filters * 8 * 8, args.dense_layer),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(args.dense_dropout),\n",
    "            nn.Linear(args.dense_layer, 10)\n",
    "        ])\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.cnn_block_1(X)\n",
    "        X = self.cnn_block_2(X)\n",
    "        X = self.flatten(X)\n",
    "        X = self.head(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "clf = CIFAR10Model()\n",
    "\n",
    "if args.from_checkpoint:\n",
    "    if args.from_checkpoint == \"latest\":\n",
    "        start_epoch = max([int(re.findall(\"[0-9]{1,2}\", fp)[0]) for fp in os.listdir(\"/mnt/checkpoints/\")])\n",
    "    else:\n",
    "        start_epoch = args.from_checkpoint\n",
    "    clf.load_state_dict(torch.load(f\"/mnt/checkpoints/epoch_{start_epoch}.pth\"))\n",
    "    print(f\"Resuming training from epoch {start_epoch}...\")\n",
    "else:\n",
    "    start_epoch = 1\n",
    "\n",
    "clf.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(clf.parameters(), lr=0.0001, weight_decay=1e-6)\n",
    "\n",
    "\n",
    "def test(epoch, num_epochs):\n",
    "    losses = []\n",
    "    n_right, n_total = 0, 0\n",
    "    clf.eval()\n",
    "\n",
    "    for i, (X_batch, y_cls) in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            y = y_cls.cuda()\n",
    "            X_batch = X_batch.cuda()\n",
    "\n",
    "            y_pred = clf(X_batch)\n",
    "            loss = criterion(y_pred, y)\n",
    "            losses.append(loss.item())\n",
    "            _, y_pred_cls = y_pred.max(1)\n",
    "            n_right, n_total = n_right + (y_pred_cls == y_cls.cuda()).sum().item(), n_total + len(X_batch)\n",
    "\n",
    "    val_acc = n_right / n_total\n",
    "    val_loss = np.mean(losses)\n",
    "\n",
    "    send_metric(\"val_loss\", val_loss)\n",
    "    send_metric(\"val_acc\", val_acc)\n",
    "    wandb.log({\"val_loss\": val_loss, \"val_acc\": val_acc})\n",
    "    print(\n",
    "        f'Finished epoch {epoch}/{num_epochs} avg val loss: {val_loss:.3f}; median val loss: {np.median(losses):.3f}; '\n",
    "        f'val acc: {val_acc:.3f}.'\n",
    "    )\n",
    "\n",
    "\n",
    "def train():\n",
    "    clf.train()\n",
    "    NUM_EPOCHS = args.epochs\n",
    "\n",
    "    wandb.init()\n",
    "    wandb.config.update(args)\n",
    "    wandb.watch(clf)\n",
    "\n",
    "    for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
    "        losses = []\n",
    "\n",
    "        for i, (X_batch, y_cls) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y = y_cls.cuda()\n",
    "            X_batch = X_batch.cuda()\n",
    "\n",
    "            y_pred = clf(X_batch)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss = loss.item()\n",
    "            if i % 200 == 0:\n",
    "                print(\n",
    "                    f'Finished epoch {epoch}/{NUM_EPOCHS}, batch {i}. loss: {train_loss:.3f}.'\n",
    "                )\n",
    "                send_metric(\"train_loss\", train_loss)\n",
    "                wandb.log({\"train_loss\": train_loss})\n",
    "            losses.append(train_loss)\n",
    "\n",
    "        print(\n",
    "            f'Finished epoch {epoch}. '\n",
    "            f'avg loss: {np.mean(losses)}; median loss: {np.median(losses)}'\n",
    "        )\n",
    "        test(epoch, NUM_EPOCHS)\n",
    "        if epoch % 5 == 0:\n",
    "            torch.save(clf.state_dict(), f\"{CWD}/checkpoints/epoch_{epoch}.pth\")\n",
    "\n",
    "    torch.save(clf.state_dict(), f\"{CWD}/checkpoints/model_final.pth\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spell run --machine-type t4 \\\n",
    "    --github-url https://github.com/spellml/cnn-cifar10.git \\\n",
    "    --tensorboard-dir /spell/tensorboard/ \\\n",
    "    --mount uploads/cifar10/:/mnt/cifar10/ -- \\\n",
    "    python models/wandb_train.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}