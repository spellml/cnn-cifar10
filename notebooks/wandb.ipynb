{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wandb\n",
    "\n",
    "This notebook is a further refinement of the model defined in `train.ipynb` that replaces TensorBoard with Spell's [Weights & Biases integration](https://spell.ml/docs/integrating_wandb/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.9.4-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.16.3)\n",
      "Collecting configparser>=3.8.1\n",
      "  Downloading configparser-5.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting watchdog>=0.8.3\n",
      "  Downloading watchdog-0.10.3.tar.gz (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 4.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
      "Collecting gql==0.2.0\n",
      "  Downloading gql-0.2.0.tar.gz (18 kB)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.24.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.7.0)\n",
      "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.3.1)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
      "Collecting nvidia-ml-py3>=7.352.0\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "Collecting subprocess32>=3.5.3\n",
      "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 9.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.25.9)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.6.20)\n",
      "Collecting pathtools>=0.1.1\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Collecting graphql-core<2,>=0.5.0\n",
      "  Downloading graphql-core-1.1.tar.gz (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.4)\n",
      "Building wheels for collected packages: watchdog, gql, nvidia-ml-py3, subprocess32, pathtools, graphql-core, promise\n",
      "  Building wheel for watchdog (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for watchdog: filename=watchdog-0.10.3-py3-none-any.whl size=73871 sha256=741b4065a3d4764b97db76f899d13994c86cb05a1b31699910c0c79b3d34fa9c\n",
      "  Stored in directory: /root/.cache/pip/wheels/27/21/35/9d1e531f9de5335147dbef07e9cc99d312525ba128a93d1225\n",
      "  Building wheel for gql (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gql: filename=gql-0.2.0-py3-none-any.whl size=7628 sha256=60242d2653d874d898ab7e79a75e825327c167ff0e1a9106ad9a2198d88bf199\n",
      "  Stored in directory: /root/.cache/pip/wheels/b6/9a/56/5456fd32264a8fc53eefcb2f74e24e99a7ef4eb40a9af5c905\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19189 sha256=a846aa49386f5295a6f1c58d609c846aaebc342006d37b87f08adcc020848df1\n",
      "  Stored in directory: /root/.cache/pip/wheels/df/99/da/c34f202dc8fd1dffd35e0ecf1a7d7f8374ca05fbcbaf974b83\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6489 sha256=1dab0b903269cf4ecf4a09b4541dc8a263d0ef33d99dbd815897200fb5c2c3b0\n",
      "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8784 sha256=905a084b5ca4405c9980995857298ae0101ca7ad00ddbc971a1f8de8a8c82638\n",
      "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
      "  Building wheel for graphql-core (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for graphql-core: filename=graphql_core-1.1-py3-none-any.whl size=104650 sha256=97f8a6f939cb639ab0fa6f9b088dc07c9ccd28256e6c091a64da37efdcef6a71\n",
      "  Stored in directory: /root/.cache/pip/wheels/6b/fd/8c/a20dd591c1a554070cc33fb58042867e6ac1c85395abe2e57a\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21495 sha256=a0b8c60a7a4b45cdcb2e49c5755db4600366137c4e1b00c6df51ef49cb7a9daa\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
      "Successfully built watchdog gql nvidia-ml-py3 subprocess32 pathtools graphql-core promise\n",
      "Installing collected packages: configparser, pathtools, watchdog, docker-pycreds, promise, graphql-core, gql, shortuuid, nvidia-ml-py3, subprocess32, wandb\n",
      "Successfully installed configparser-5.0.0 docker-pycreds-0.4.0 gql-0.2.0 graphql-core-1.1 nvidia-ml-py3-7.352.0 pathtools-0.1.2 promise-2.3 shortuuid-1.0.1 subprocess32-3.5.4 wandb-0.9.4 watchdog-0.10.3\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../models/wandb_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../models/wandb_train.py\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "from spell.metrics import send_metric\n",
    "\n",
    "import re\n",
    "import os\n",
    "if not os.path.exists(\"/spell/checkpoints/\"):\n",
    "    os.mkdir(\"/spell/checkpoints/\")\n",
    "writer = SummaryWriter('/spell/tensorboard/')\n",
    "\n",
    "import wandb\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--epochs', type=int, dest='epochs', default=50)\n",
    "# parser.add_argument('--batch_size', type=int, dest='batch_size', default=32)\n",
    "\n",
    "# parser.add_argument('--conv1_filters', type=int, dest='conv1_filters', default=32)\n",
    "# parser.add_argument('--conv2_filters', type=int, dest='conv2_filters', default=64)\n",
    "# parser.add_argument('--dense_layer', type=int, dest='dense_layer', default=512)\n",
    "\n",
    "# parser.add_argument('--conv1_dropout', type=float, dest='conv1_dropout', default=0.25)\n",
    "# parser.add_argument('--conv2_dropout', type=float, dest='conv2_dropout', default=0.25)\n",
    "# parser.add_argument('--dense_dropout', type=float, dest='dense_dropout', default=0.5)\n",
    "\n",
    "# parser.add_argument('--from_checkpoint', type=str, dest='from_checkpoint', default=\"\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# Used for testing purposes.\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.epochs = 50\n",
    "        self.batch_size = 32\n",
    "        self.conv1_filters = 32\n",
    "        self.conv2_filters = 64\n",
    "        self.dense_layer = 512\n",
    "        self.conv1_dropout = 0.25\n",
    "        self.conv2_dropout = 0.25\n",
    "        self.dense_dropout = 0.5\n",
    "        self.from_checkpoint = False\n",
    "args = Args()\n",
    "\n",
    "transform_train = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    # torchvision.transforms.Lambda(lambda x: torch.tensor(np.array(x).reshape((3, 32, 32)) / 255, dtype=torch.float)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "transform_test = torchvision.transforms.Compose([\n",
    "    # torchvision.transforms.Lambda(lambda x: torch.tensor(np.array(x).reshape((3, 32, 32)) / 255, dtype=torch.float)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),    \n",
    "])\n",
    "\n",
    "download = not os.path.exists(\"/mnt/cifar10/\")\n",
    "if download:\n",
    "    print(\"CIFAR10 dataset not on disk, downloading...\")\n",
    "else:\n",
    "    print(\"CIFAR10 dataset is already on disk! Skipping download.\")\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\"/mnt/cifar10/\", train=True, transform=transform_train, download=download)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "# download=False because the train and test sets are downloaded simultaneously\n",
    "test_dataset = torchvision.datasets.CIFAR10(\"/mnt/cifar10/\", train=False, transform=transform_test, download=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "class CIFAR10Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn_block_1 = nn.Sequential(*[\n",
    "            nn.Conv2d(3, args.conv1_filters, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(args.conv1_filters, args.conv2_filters, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(args.conv1_dropout)\n",
    "        ])\n",
    "        self.cnn_block_2 = nn.Sequential(*[\n",
    "            nn.Conv2d(args.conv2_filters, args.conv2_filters, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(args.conv2_filters, args.conv2_filters, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(args.conv2_dropout)\n",
    "        ])\n",
    "        self.flatten = lambda inp: torch.flatten(inp, 1)\n",
    "        self.head = nn.Sequential(*[\n",
    "            nn.Linear(args.conv2_filters * 8 * 8, args.dense_layer),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(args.dense_dropout),\n",
    "            nn.Linear(args.dense_layer, 10)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.cnn_block_1(X)\n",
    "        X = self.cnn_block_2(X)\n",
    "        X = self.flatten(X)\n",
    "        X = self.head(X)\n",
    "        return X\n",
    "\n",
    "clf = CIFAR10Model()\n",
    "\n",
    "if args.from_checkpoint:\n",
    "    if args.from_checkpoint == \"latest\":\n",
    "        start_epoch = max([int(re.findall(\"[0-9]{1,2}\", fp)[0]) for fp in os.listdir(\"/mnt/checkpoints/\")])\n",
    "    else:\n",
    "        start_epoch = args.from_checkpoint\n",
    "    clf.load_state_dict(torch.load(f\"/mnt/checkpoints/epoch_{start_epoch}.pth\"))\n",
    "    print(f\"Resuming training from epoch {start_epoch}...\")\n",
    "else:\n",
    "    start_epoch = 1\n",
    "\n",
    "clf.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(clf.parameters(), lr=0.0001, weight_decay=1e-6)\n",
    "def test(epoch, num_epochs):\n",
    "    losses = []\n",
    "    n_right, n_total = 0, 0\n",
    "    clf.eval()\n",
    "    \n",
    "    for i, (X_batch, y_cls) in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            y = y_cls.cuda()\n",
    "            X_batch = X_batch.cuda()\n",
    "\n",
    "            y_pred = clf(X_batch)\n",
    "            loss = criterion(y_pred, y)\n",
    "            losses.append(loss.item())\n",
    "            _, y_pred_cls = y_pred.max(1)\n",
    "            n_right, n_total = n_right + (y_pred_cls == y_cls.cuda()).sum().item(), n_total + len(X_batch)\n",
    "    \n",
    "    val_acc = n_right / n_total\n",
    "    val_loss = np.mean(losses)\n",
    "    \n",
    "    send_metric(\"val_loss\", val_loss)\n",
    "    send_metric(\"val_acc\", val_acc)\n",
    "    wandb.log({\"val_loss\": val_loss, \"val_acc\": val_acc})\n",
    "    print(\n",
    "        f'Finished epoch {epoch}/{num_epochs} avg val loss: {val_loss:.3f}; median val loss: {np.median(losses):.3f}; '\n",
    "        f'val acc: {val_acc:.3f}.'\n",
    "    )\n",
    "\n",
    "def train():\n",
    "    clf.train()\n",
    "    NUM_EPOCHS = args.epochs\n",
    "    \n",
    "    wandb.init()\n",
    "    wandb.config.update(args)\n",
    "    wandb.watch(clf)\n",
    "    \n",
    "    for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
    "        losses = []\n",
    "\n",
    "        for i, (X_batch, y_cls) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y = y_cls.cuda()\n",
    "            X_batch = X_batch.cuda()\n",
    "\n",
    "            y_pred = clf(X_batch)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss = loss.item()\n",
    "            if i % 200 == 0:\n",
    "                print(\n",
    "                    f'Finished epoch {epoch}/{NUM_EPOCHS}, batch {i}. loss: {train_loss:.3f}.'\n",
    "                )\n",
    "                send_metric(\"train_loss\", train_loss)\n",
    "                wandb.log({\"train_loss\": train_loss})\n",
    "            losses.append(train_loss)\n",
    "\n",
    "        print(\n",
    "            f'Finished epoch {epoch}. '\n",
    "            f'avg loss: {np.mean(losses)}; median loss: {np.median(losses)}'\n",
    "        )\n",
    "        test(epoch, NUM_EPOCHS)    \n",
    "        if epoch % 5 == 0:\n",
    "            torch.save(clf.state_dict(), f\"/spell/checkpoints/epoch_{epoch}.pth\")\n",
    "    \n",
    "    torch.save(clf.state_dict(), f\"/spell/checkpoints/epoch_{NUM_EPOCHS}.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
