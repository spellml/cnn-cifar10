{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distributed\n",
    "\n",
    "This is a further refinement to the `train.py` model training script in `train.ipynb` that adds support for distributed training using `horovod`. You can train this model on Spell in one click using our Horovod integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../models/distributed_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../models/distributed_train.py\n",
    "import re\n",
    "import os\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "from spell.metrics import send_metric\n",
    "\n",
    "import horovod.torch as hvd\n",
    "hvd.init()\n",
    "\n",
    "if hvd.local_rank() == 0:\n",
    "    CWD = os.environ[\"PWD\"]\n",
    "    if not os.path.exists(f\"{CWD}/checkpoints/\"):\n",
    "        os.mkdir(f\"{CWD}/checkpoints/\")\n",
    "if hvd.rank() == 0:\n",
    "    writer = SummaryWriter(f\"{CWD}/tensorboard/\")\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', type=int, dest='epochs', default=20)\n",
    "parser.add_argument('--batch_size', type=int, dest='batch_size', default=32)\n",
    "\n",
    "parser.add_argument('--conv1_filters', type=int, dest='conv1_filters', default=32)\n",
    "parser.add_argument('--conv2_filters', type=int, dest='conv2_filters', default=64)\n",
    "parser.add_argument('--dense_layer', type=int, dest='dense_layer', default=512)\n",
    "\n",
    "parser.add_argument('--conv1_dropout', type=float, dest='conv1_dropout', default=0.25)\n",
    "parser.add_argument('--conv2_dropout', type=float, dest='conv2_dropout', default=0.25)\n",
    "parser.add_argument('--dense_dropout', type=float, dest='dense_dropout', default=0.5)\n",
    "\n",
    "parser.add_argument('--from_checkpoint', type=str, dest='from_checkpoint', default=\"\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Used for testing purposes.\n",
    "# class Args:\n",
    "#     def __init__(self):\n",
    "#         self.epochs = 50\n",
    "#         self.batch_size = 32\n",
    "#         self.conv1_filters = 32\n",
    "#         self.conv2_filters = 64\n",
    "#         self.dense_layer = 512\n",
    "#         self.conv1_dropout = 0.25\n",
    "#         self.conv2_dropout = 0.25\n",
    "#         self.dense_dropout = 0.5\n",
    "#         self.from_checkpoint = False\n",
    "# args = Args()\n",
    "\n",
    "transform_train = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    # torchvision.transforms.Lambda(lambda x: torch.tensor(np.array(x).reshape((3, 32, 32)) / 255, dtype=torch.float)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "transform_test = torchvision.transforms.Compose([\n",
    "    # torchvision.transforms.Lambda(lambda x: torch.tensor(np.array(x).reshape((3, 32, 32)) / 255, dtype=torch.float)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),    \n",
    "])\n",
    "\n",
    "if hvd.local_rank() == 0:\n",
    "    download = not os.path.exists(\"/mnt/cifar10/\")\n",
    "    if download:\n",
    "        print(\"CIFAR10 dataset not on disk, downloading...\")\n",
    "        # initializing the dataset object downloads the dataset as a side effect\n",
    "        _ = torchvision.datasets.CIFAR10(\"/mnt/cifar10/\", download=True)\n",
    "    else:\n",
    "        print(\"CIFAR10 dataset is already on disk! Skipping download.\")\n",
    "# allow master process to catch up with worker processes post-download\n",
    "hvd.join()\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\"/mnt/cifar10/\", train=True, transform=transform_train, download=False)\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=train_sampler)\n",
    "test_dataset = torchvision.datasets.CIFAR10(\"/mnt/cifar10/\", train=False, transform=transform_test, download=False)\n",
    "test_sampler = torch.utils.data.distributed.DistributedSampler(test_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size)\n",
    "\n",
    "\n",
    "class CIFAR10Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn_block_1 = nn.Sequential(*[\n",
    "            nn.Conv2d(3, args.conv1_filters, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(args.conv1_filters, args.conv2_filters, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(args.conv1_dropout)\n",
    "        ])\n",
    "        self.cnn_block_2 = nn.Sequential(*[\n",
    "            nn.Conv2d(args.conv2_filters, args.conv2_filters, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(args.conv2_filters, args.conv2_filters, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(args.conv2_dropout)\n",
    "        ])\n",
    "        self.flatten = lambda inp: torch.flatten(inp, 1)\n",
    "        self.head = nn.Sequential(*[\n",
    "            nn.Linear(args.conv2_filters * 8 * 8, args.dense_layer),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(args.dense_dropout),\n",
    "            nn.Linear(args.dense_layer, 10)\n",
    "        ])\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.cnn_block_1(X)\n",
    "        X = self.cnn_block_2(X)\n",
    "        X = self.flatten(X)\n",
    "        X = self.head(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "clf = CIFAR10Model()\n",
    "\n",
    "if args.from_checkpoint:\n",
    "    if args.from_checkpoint == \"latest\":\n",
    "        start_epoch = max([int(re.findall(\"[0-9]{1,2}\", fp)[0]) for fp in os.listdir(\"/mnt/checkpoints/\")])\n",
    "    else:\n",
    "        start_epoch = args.from_checkpoint\n",
    "    clf.load_state_dict(torch.load(f\"/mnt/checkpoints/epoch_{start_epoch}.pth\"))\n",
    "    if hvd.local_rank() == 0:\n",
    "        print(f\"Resuming training from epoch {start_epoch}...\")\n",
    "else:\n",
    "    start_epoch = 1\n",
    "\n",
    "clf.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(clf.parameters(), lr=0.0001 * hvd.size(), weight_decay=1e-6)\n",
    "optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=clf.named_parameters(), op=hvd.Average)\n",
    "\n",
    "\n",
    "def test(epoch, num_epochs):\n",
    "    losses = []\n",
    "    n_right, n_total = 0, 0\n",
    "    clf.eval()\n",
    "\n",
    "    for i, (X_batch, y_cls) in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            y = y_cls.cuda()\n",
    "            X_batch = X_batch.cuda()\n",
    "\n",
    "            y_pred = clf(X_batch)\n",
    "            loss = criterion(y_pred, y)\n",
    "            losses.append(loss.item())\n",
    "            _, y_pred_cls = y_pred.max(1)\n",
    "            n_right, n_total = n_right + (y_pred_cls == y_cls.cuda()).sum().item(), n_total + len(X_batch)\n",
    "\n",
    "    val_acc = n_right / n_total\n",
    "    val_loss = np.mean(losses)\n",
    "    send_metric(\"val_loss\", val_loss)\n",
    "    send_metric(\"val_acc\", val_acc)\n",
    "    writer.add_scalar(\"val_loss\", val_loss, (len(train_dataloader) // 200 + 1) * epoch + (i // 200))\n",
    "    writer.add_scalar(\"val_acc\", val_acc, (len(train_dataloader) // 200 + 1) * epoch + (i // 200))\n",
    "    print(\n",
    "        f'Finished epoch {epoch}/{num_epochs} avg val loss: {val_loss:.3f}; median val loss: {np.median(losses):.3f}; '\n",
    "        f'val acc: {val_acc:.3f}.'\n",
    "    )\n",
    "\n",
    "\n",
    "def train():\n",
    "    torch.cuda.set_device(hvd.local_rank())\n",
    "    torch.set_num_threads(1)\n",
    "    clf.train()\n",
    "\n",
    "    NUM_EPOCHS = args.epochs\n",
    "\n",
    "    for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        test_sampler.set_epoch(epoch)\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        for i, (X_batch, y_cls) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y = y_cls.cuda()\n",
    "            X_batch = X_batch.cuda()\n",
    "\n",
    "            y_pred = clf(X_batch)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss = loss.item()\n",
    "            if hvd.rank() == 0:\n",
    "                if i % 100 == 0:\n",
    "                    print(\n",
    "                        f'Finished epoch {epoch}/{NUM_EPOCHS}, batch {i}. loss: {train_loss:.3f}.'\n",
    "                    )\n",
    "                    send_metric(\"train_loss\", train_loss)\n",
    "                    writer.add_scalar(\"train_loss\", train_loss, (len(train_dataloader) // 200 + 1) * epoch + (i // 200))\n",
    "            losses.append(train_loss)\n",
    "\n",
    "        if hvd.rank() == 0:\n",
    "            print(\n",
    "                f'Finished epoch {epoch}. '\n",
    "                f'avg loss: {np.mean(losses)}; median loss: {np.median(losses)}'\n",
    "            )\n",
    "            test(epoch, NUM_EPOCHS)\n",
    "            if epoch % 5 == 0:\n",
    "                torch.save(clf.state_dict(), f\"/spell/checkpoints/epoch_{epoch}.pth\")\n",
    "\n",
    "    if hvd.rank() == 0:\n",
    "        torch.save(clf.state_dict(), f\"/spell/checkpoints/epoch_{NUM_EPOCHS}.pth\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mðŸ’« Casting spell #120â€¦\n",
      "\u001b[0mâœ¨ Stop viewing logs with ^C\n",
      "\u001b[0m^C\n",
      "\n",
      "\u001b[0mâœ¨ Your run is still running remotely.\n",
      "\u001b[0mâœ¨ Use 'spell kill 120' to terminate your run\n",
      "\u001b[0mâœ¨ Use 'spell logs 120' to view logs again\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!spell run --machine-type t4 \\\n",
    "    --github-url https://github.com/spellml/cnn-cifar10.git \\\n",
    "    --tensorboard-dir /spell/tensorboard/ \\\n",
    "    --distributed 1 -- \\\n",
    "    python models/distributed_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mðŸ’« Casting spell #121â€¦\n",
      "\u001b[0mâœ¨ Stop viewing logs with ^C\n",
      "\u001b[1m\u001b[36mðŸŒŸ\u001b[0m Machine_Requestedâ€¦ Run created -- waiting for a t4 machine.\u001b[0m^C\n",
      "\n",
      "\u001b[0mâœ¨ Your run is still running remotely.\n",
      "\u001b[0mâœ¨ Use 'spell kill 121' to terminate your run\n",
      "\u001b[0mâœ¨ Use 'spell logs 121' to view logs again\n",
      "\u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!spell run --machine-type t4 \\\n",
    "    --github-url https://github.com/spellml/cnn-cifar10.git \\\n",
    "    --tensorboard-dir /spell/tensorboard/ \\\n",
    "    --distributed 4 -- \\\n",
    "    python models/distributed_train.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}